\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{csquotes}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Security of OpenSSL Tool}

\author{\IEEEauthorblockN{John Phillips}
\textit{johphill@mines.edu}\\
\and
\IEEEauthorblockN{2\textsuperscript{nd} Abhaya Shrestha}
\IEEEauthorblockA{\textit{ashrestha@mines.edu}}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Joe Granmoe}
\IEEEauthorblockA{\textit{jgranmoe@mines.edu}}
\and
\IEEEauthorblockN{4\textsuperscript{th} Patrick Curran}
\IEEEauthorblockA{\textit{pcurran@mines.edu}}
\and
\IEEEauthorblockN{5\textsuperscript{th} Collin McDade}
\IEEEauthorblockA{\textit{collinmcdade@mines.edu}}
\and
\IEEEauthorblockN{6\textsuperscript{th} Noor Malik}
\IEEEauthorblockA{\textit{nmalik@mines.edu}}
}

\maketitle

\begin{abstract}
There have been small but catastrophic modifications to openssl in the
past\cite{1}\cite{2}\cite{5} that cause massive security problems and
in some cases exploits that can disclose private keys. We explore in
depth one such instance, a modification made to the OpenSSL package
distributed in debian that resulted in only 32,768 possible keys that
could be generated. We also explore similarities to
heartbleed. Finally, we investigate methods that have been put in
place to prevent such bugs from happening in the future.
\end{abstract}

\begin{IEEEkeywords}
OpenSSL, security vulnerability, debian
\end{IEEEkeywords}

\section{Introduction}
\section{Background}
In 2008 a debian developer made a patch to openssl to fix valgrind
warnings\cite{2}\cite{3}. OpenSSL has a flexible system for generating
random numbers that uses an interface-like approach with functions
prefixed with \verb|RAND_*| delegating to a particular
implementation. Since this API was designed for cryptographic
applications, the random bytes generated need to not just satisfy
particular statistical properties but be truly unpredictable. In order
to acheive this property, this particular random number generator
allows the user to add 'entropy' by feeding it buffers full of random
bytes. These bytes could be from \verb|/dev/urandom|, the user, or
somewhere else. When adding entropy in this way to the message
digest-based random API, an internal hash state is update each time
with the random bytes given by the user. The hash state then can be
used to get random bytes back out of the system. 

Now this particular implementation in \verb|md_rand.c|,
\emph{deliberately} would `over-read' the user-provided buffer. The
apparent thinking behind this was that if the code read uninitialized
memory, this couldn't hurt things, and could only possibly help by
potentially adding randomness.

This behavior occurred in two places in \verb|md_rand.c|. Both of
those places were actually critical to the entire random number
generator. Because of the reading of uninitialized memory, any time a
user of the ssl library did almost anything -- like generate a key,
for example -- they would get disturbing warnings if they ran their
program in the popular \verb|valgrind| tool.

Most linux distributions maintain small set of `patches' for each
package they distribute. These patches normally are designed to fix
critical bugs that haven't been fixed upstream or are in the process
of being fixed. So although the debian project doesn't maintain
OpenSSL, they could patch the particular version of OpenSSL
distributed by debian. And that was the solution arrived at in the bug
ticket and over email \cite{2}\cite{4}.

This was the source of a complaint that first came to the debian bug
tracker\cite{2}. The debian developer quickly spotted the culprit code
but didn't know what it did. Observing that commenting it out didn't
cause anything to crash or obviously function incorrectly, he asked on
the openssl development mailing list if it was okay to remove the two
lines of code -- and got the go-ahead.

\begin{displayquote}
\begin{verbatim}
List:       openssl-dev
Subject:    Re: Random number generator, uninitialised data and valgrind.
From:       Ulf_MÃ¶ller <ulf () openssl ! org>
Date:       2006-05-01 22:34:12
Message-ID: 44568CE4.9020906 () openssl ! org
[Download RAW message or body]

Kurt Roeckx schrieb:
> What I currently see as best option is to actually comment out
> those 2 lines of code.  But I have no idea what effect this
> really has on the RNG.  The only effect I see is that the pool
> might receive less entropy.  But on the other hand, I'm not even
> sure how much entropy some unitialised data has.
>   
Not much. If it helps with debugging, I'm in favor of removing them. 
(However the last time I checked, valgrind reported thousands of bogus 
error messages. Has that situation gotten better?)
\end{verbatim}
\end{displayquote}
\cite{4}

The debian developer then applied the patch and distributed the debian
version of OpenSSL with this critical vulnerability built in. This
vulnerability was distributed to all users of debian from 2006 to
2008\cite{1}. During that entire time, any code using the random
number generator could only really have 32,767 possible outputs from
the random number generator. Examples of affected functions included
TLS modules and SSH key generation -- actually, anything that
generated RSA keys was vulnerable.

\section{Recreating the 2008 bug}

In order to gain a better understanding of exactly how the bug worked,
and how easy (or difficult) it would be to exploit such a flaw, we
used virtual machines and code from the 2008 era to see what
exploiting the vulnerability might look like. The basic idea was to
create two virtual machines. One would be the exploited machine -- a
stand-in for a server on the Internet or other network that had an
open SSH port that was believed to be secure. The other machine would
try to access the target server over SSH by repeatedly generating
trial keys until it was able to successfully get in. Although this is
not as simple as just writing code to guess the key and compares two
files, it is arguably a more realistic demonstration of how exploits
really work, and a better way of understanding them, putting the
`applied' in `applied cryptography'.

\subsection{Code modifications}
First, we created two different versions of the OpenSSL code. The
first one had the two lines that the debian developer asked about on
the OpenSSL mailling list commented out (see \cite{4}), and nothing
more. It was a re-creation of what existed in the debian project from
2006-2008. We looked for the exact code (for example, a .deb binary
distribution, or the actual patchsets applied to the OpenSSL project)
on the Internet but were unable to find it (links pointing to the
patches were all dead, possibly in a deliberate attempt by debian to prevent
anyone accidentally using the code again).

The next branch of the OpenSSL code was similar to the first except it
had a way to manually set the only real source of variablility left to
the random number generator -- the PID (process ID). We did this by
replacing calls to \verb|getpid()| in \verb|md_rand.c| to references
to a static variable in \verb|md_rand.c| called \verb|fake_pid|. We
also introduced a globally-visible function called
\verb|RAND_exploit_set_pid()|, to allow client code to set the PID.


\subsection{Virtual machine setup}
We built two virtual machines based on debian 5. The machines were
connected to a single virtual switch, and had static IPs assigned. On
one virtual machine, we built and installed the re-created
debian-distributed OpenSSL. We used \verb|ssh-keygen| to create and
authorize a key as an authorized SSH key on that host.

On the other (attacker) machine, we used a program derived from
\verb|ssh-keygen| to try to SSH to the target machine with a given PID
seed. That program then was repeatedly invoked by another script that
iterated over all possible PIDs, from 1 to 32,768 (in 2008 that was
the number of possible PIDs).

The results were as expected -- the attacker machine was able to
generate the public/private key pair of the target machine within
fifteen to twenty minutes. On a fresh boot, because there haven't been
as many processes run, it takes only about five minutes.

\subsection{Running the exploit}

\subsection{Alternative approaches}

\section{Similarities to Heartbleed}
The debian bug bears certain similarities to another bug that occurred
in 2014 -- Heartbleed\cite{4}. Both bugs compromised private keys, and
both were not the result of any protocol design errors, cryptanalytic
breakthroughs, or any other issue commonly learned about in a
cryptography class. Instead, both errors were the result of human
error. Simple, and (in hindsight) rather obvious bugs made their way
into OpenSSL packages that were distributed to <how many>
machines<CITE!>.

Both bugs were the result of reading into memory regions that were not
intended to be read into, which wouldn't even be possible in many
other languages.

This raises the question of how, if possible, to catch such simple
implementation errors?

\section{Investigating the current code}

\section{Processes in the OpenSSL project in place to prevent further exploits}
   Exploring OpenSSL project contribution process,
we looked into what it takes to be a committer in their repo, whether
they have any kind of automated checking in place (CI or Continuous Integration),
and what their approval process looks like.
    Before we delve into the process, we need to define a few terms from
their glossary.
    \begin{enumerate}
        \item \textbf{OMC (OpenSSL Management Committee)}: they oversee all
        managerial and administrative aspects of the project. They are
        the final authority for the project\cite{6}
        \item \textbf{OTC (OpenSSL Technical Committee)}: they oversee all
        technical aspects of the project\cite{6}.
    \end{enumerate}
\subsection{What it takes to be a Commiter of OpenSSL?}
    To become a committer, one has to be granted access by the OMC on
the recommendation of OTC\cite{7}. This access can be withdrawn at any time by
a vote of the OMC\cite{8}. In order to maintain their committer status,
a commiter must have authored or reviewed at least one commit within the
previous two calendar quarters\cite{8}.
    They expect committers to be experts in some part of low-level
crypto library as well as generalists who contribute to all areas of
codebase\cite{7}. Committers are expected to oversee the health of the
project: fixing bugs, addressing open issues, reviewing contributions, and
improving tests and documentation\cite{7}. According to OpenSSL, to be a commiter,
one can start by contributing code, reading code style, and getting to know
build and test system\cite{7}. In short, anyone can be a
commiter as long as they are granted access by OMC on recommendation of OTC.
Their contributions can range from small documentation fixes, spelling errors,
to large code base contributions.

\subsection{What automation checks does OpenSSL have to ensure code safety?}
    Based off of their glossery, we were able to understand that OpenSSL uses
Continuous Integration which is a suite of tests and checks that are run on
every pull request, commit on a daily basis. Some of their tests include functional
tests, performance testing, and fuzz testing\cite{9}. These kinds of tests
are described as follows.
    \begin{enumerate}
        \item \textbf{Functional Tests}: given a set of inputs, it
        will produce a correct set of outputs\cite{9}.
        \item \textbf{Performance Tests}: These tests test
        performance, and will be performed automatically
        via CI on a regular basis for certain components. Examples include:
        \begin{itemize}
            \item Individual algorithm performance operating over different
            input sizes
            \item SSL/TLS handshake time over multiple handshakes and
            for different protocol versions and resumption/non-resumption
            handshake.
        \end{itemize}\cite{9}.
        \item \textbf{Fuzz Tests}: Systematic methodology that is used
        to find buffer overruns (remote execution); unhandled
        exceptions, read access violations (AVs), and thread hangs
        (permanent denial-of-service), and memory spikes (temporary denial-of-service)\cite{10}.
    \end{enumerate}
According to their testing policy, a pull request does not require testing
if the changes are as follows\cite{9}.
\begin{itemize}
    \item Documentation (including CHANGES/NEWS file changes)
    \item The test suite
    \item perl utilities
    \item include files
    \item build system
    \item demos
\end{itemize}
Additionally, they recommend adding fuzz testing when implementing significant new functional tests
or at least consider adding fuzz testing when refactoring to check all corner cases are covered\cite{9}.
OpenSSL particulary uses either LibFuzzer or AFL to do fuzz testing \cite{11}. While performance
tests improve the functionality of OpenSSL, fuzz testing and functional testing helped us ensure
that OpenSSL codebase is at least sanity checked. Fuzz tests especially help since they are
basically dynamic analysis detection for any vulnerabilities or bugs in the codebase.
\subsection{What does their approval and code review process look like?}
    OpenSSL approval and code reviews process are as follows.
    \begin{itemize}
        \item OpenSSL pull requests are reviewed and approved by at least two committers, one of
        whom must be an OTC member. Neither of the reviewers can be the author of the submission\cite{7}.
        Only exception to this policy is during the release process when the author's review does
        count towards the two needed reviews for the automated release process and NEWS and CHANGES
        file updates\cite{7}.
        \item In case two committers make a joint submission, they can review each other's
        code, but not their own. Additionally, a third reviewer will be required\cite{7}.
        \item An OMC/OTC member may apply a needs OMC decision label to a submission which
        can hold the submission. This hold may only be removed by OTC/OMC member who
        put the hold or the OTC/OMC\cite{7}.
        \item Approved submission (besides autmated release process and NEWS and CHANGES
        file updates) will only be applied
        after 24-hour delay from approval. An exception to the delay exists
        for build and test breakage fix approvals which will be flagged with the
        severity: urgent label\cite{7}.
    \end{itemize}
    Additionally, on their commit workflow, their public github repository is just a mirror
    and pulls from the real repo where only committers have
    access\cite{7} (When someone becomes a committer, OpenSSL will
    send instructions to get commit access to the repository\cite{7}). 
    Finally, they also have strict documentation design process whenever 
    minor or major code changes the design, and
    changes to existing public API functions and data are forbidden
    \cite{9}. However, they encourage a new API call to be implemented
    if need be as a minor release \cite{9}. From this information, we
    can definitely tell that today it is difficult to be able to commit
    anything without prior granted permissions and approval to OpenSSL. Futhermore,
    even if changes were to occur, they always run their automation
    process to detect any defects or vulnerabilities with their test suites which
    is very assuring for a user of OpenSSL.\\
    \quad However, there are still chances that
    a defect or vulnerability could be shipped out. Particularly, when
    they suggested that refactors do not need a test but
    recommend Fuzz testing to cover edge cases. In the
    refactor case, there is a possibility that
    the committer misses an edge case in fuzz
    testing and creates a bug or vulnerability.
    Another challenge of a cryptographic tool
    such as OpenSSL is that professionals do not know how to
    test for security. An algorithm implementation is
    only secure if an attacker cannot decipher or find
    vulnerabilities to exploit for the algorithm. Futhermore,
    Fuzz tests are basically just shooting in the dark
    and hoping to find a vulnerability before
    the adversary finds it.
    Nonetheless, we have to say their process
    for who gets to commit code to the master is solid; however,
    there is no assurance that vulnerability will not be shipped out.
    The best we could do is hope that the code review and automation
    process detects a defect or vulerability and prevents it from being
    shipped out.
\begin{thebibliography}{00}
\bibitem{1} \verb|https://isotoma.com/blog/2008/05/14/debians-openssl-disaster/|
\bibitem{2} \verb|https://bugs.debian.org/cgi-bin/bugreport.cgi?bug=363516|
\bibitem{3} \verb|https://research.swtch.com/openssl|
\bibitem{4} \verb|https://marc.info/?l=openssl-dev&m=114652287210110&w=2|
\bibitem{5}  \verb|https://heartbleed.com/|
\bibitem{6} \verb|https://www.openssl.org/policies/glossary.html|
\bibitem{7} \verb| https://www.openssl.org/policies/general/committer-policy.html|
\bibitem{8} \verb|https://www.openssl.org/policies/omc-bylaws.html|
\bibitem{9} \verb|https://www.openssl.org/policies/technical/|
\bibitem{10} \verb|https://docs.microsoft.com/en-us/previous-versions/software-testing/cc162782(v=msdn.10)?redirectedfrom=MSDN|
\bibitem{11} \verb|https://github.com/openssl/openssl/tree/master/fuzz|
\end{thebibliography}

\end{document}
